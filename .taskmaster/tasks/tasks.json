{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Establish Test Infrastructure and TDD Foundation",
        "description": "Set up helm-unittest, OPA/Conftest, CI/CD pipeline, and test directory structure before any template development",
        "details": "Install helm-unittest plugin (v3.14.0+). Create test directory structure: tests/unit/, tests/integration/, tests/values/. Install Conftest (v0.45.0+) and create policies/ directory with base security policies (deny privileged containers, deny latest tags, deny missing resource limits, warn on missing labels/probes). Create .github/workflows/ci.yaml with jobs for: helm lint, helm unittest, conftest verify. Create values.schema.json skeleton following JSON Schema Draft 7. Write example test files demonstrating: template rendering tests, snapshot tests, assertion patterns. Create CONTRIBUTING.md documenting TDD workflow: write test first, implement template, verify test passes, commit both. Set up pre-commit hooks to run tests locally.",
        "testStrategy": "Verify helm unittest runs successfully with zero templates. Verify CI pipeline executes and passes. Verify Conftest policies validate sample manifests. Verify values.schema.json validates against sample values. Test that CI fails when lint/test/policy checks fail.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Install helm-unittest and create test directory structure",
            "description": "Install the helm-unittest plugin (v3.14.0+) and establish the foundational test directory hierarchy for unit, integration, and values tests",
            "dependencies": [],
            "details": "Install helm-unittest plugin using 'helm plugin install https://github.com/helm-unittest/helm-unittest.git'. Create directory structure: tests/unit/ for template rendering tests, tests/integration/ for end-to-end scenario tests, tests/values/ for values file validation tests. Verify plugin installation with 'helm unittest --help'. Create a basic .helmignore file to exclude test directories from packaged charts.",
            "status": "done",
            "testStrategy": "Run 'helm unittest --help' to verify plugin is installed correctly. Execute 'helm unittest .' in chart root to verify it runs without errors (should report 0 tests). Verify directory structure exists with 'ls -la tests/' showing unit/, integration/, and values/ subdirectories.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Set up Conftest/OPA with base security policies",
            "description": "Install Conftest (v0.45.0+) and create a policies/ directory with foundational OPA security policies for container security best practices",
            "dependencies": [],
            "details": "Install Conftest v0.45.0+ via package manager or GitHub releases. Create policies/ directory in chart root. Implement base OPA policies in policies/security.rego: deny privileged containers (deny[msg] when input.kind == 'Deployment' and container.securityContext.privileged == true), deny latest image tags (deny when image endsWith ':latest'), deny missing resource limits (deny when missing resources.limits), warn on missing health probes (warn when missing livenessProbe or readinessProbe), warn on missing recommended labels (warn when missing app.kubernetes.io/name, app.kubernetes.io/version). Create policies/policy-metadata.yaml documenting each policy's purpose and severity.",
            "status": "done",
            "testStrategy": "Verify Conftest installation with 'conftest --version' showing v0.45.0+. Create sample deployment manifest with violations (privileged container, latest tag, no limits) and run 'conftest test sample.yaml' to verify policies detect violations. Verify policies return expected deny/warn messages. Test against compliant manifest to ensure no false positives.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create CI/CD pipeline with GitHub Actions",
            "description": "Implement .github/workflows/ci.yaml with automated jobs for helm lint, helm unittest, and conftest policy verification",
            "dependencies": [
              1,
              2
            ],
            "details": "Create .github/workflows/ci.yaml with trigger on push/pull_request to main branch. Define three jobs: 'lint' job running 'helm lint .' on ubuntu-latest with Helm 3.14+ installed, 'unittest' job installing helm-unittest plugin and running 'helm unittest --color --update-snapshot .', 'policy' job installing Conftest and running 'conftest test <(helm template . --values values.yaml)' to verify rendered manifests against OPA policies. Use helm/chart-testing-action@v2 for standardized Helm CI. Configure job dependencies so unittest runs after lint passes, policy runs after unittest passes. Add status badge to README.md linking to workflow.",
            "status": "done",
            "testStrategy": "Push workflow file to GitHub and verify all three jobs execute successfully. Introduce intentional lint error (malformed YAML) and verify lint job fails. Introduce policy violation (privileged container in deployment) and verify policy job fails with expected error message. Verify workflow badge appears correctly in README.md and shows passing status.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create values.schema.json skeleton and validation setup",
            "description": "Implement JSON Schema Draft 7 schema file for values.yaml validation with core field definitions and CI validation integration",
            "dependencies": [],
            "details": "Create values.schema.json following JSON Schema Draft 7 specification. Define root schema with $schema property set to 'http://json-schema.org/draft-07/schema#'. Implement schema structure mirroring values.yaml hierarchy: replicaCount (type: integer, minimum: 1), image (object with repository, tag, pullPolicy properties), config (object with datastoreEngine enum, logLevel enum), resources (object following Kubernetes resource spec), securityContext (object following Kubernetes SecurityContext spec). Set required fields and provide property descriptions. Add schema validation to CI pipeline using 'helm lint --strict' which automatically validates against values.schema.json if present. Create values-examples/ directory with valid and invalid values files for schema testing.",
            "status": "done",
            "testStrategy": "Validate values.schema.json is well-formed JSON with 'jq . values.schema.json'. Create values-invalid.yaml with schema violations (replicaCount: 0, invalid datastoreEngine) and verify 'helm lint --strict' fails with schema validation errors. Create values-valid.yaml conforming to schema and verify 'helm lint --strict' passes. Test that missing required fields are caught by validation.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Write example test files demonstrating test patterns",
            "description": "Create comprehensive example test files in tests/unit/ showcasing template rendering tests, snapshot tests, and assertion patterns for helm-unittest",
            "dependencies": [
              1
            ],
            "details": "Create tests/unit/deployment_test.yaml demonstrating: basic template rendering test (set values, assert manifest renders without error), snapshot test (capture rendered output, detect unexpected changes), assertion patterns (assertEqual for specific values, assertMatchRegex for patterns, assertExists/assertNotExists for conditional resources). Create tests/unit/service_test.yaml showing: port configuration tests, selector label tests, service type tests (ClusterIP, LoadBalancer, NodePort). Create tests/unit/helpers_test.yaml testing _helpers.tpl functions: spicedb.fullname, spicedb.labels, spicedb.selectorLabels. Include comments explaining each test pattern. Demonstrate testing conditional logic (if .Values.ingress.enabled) and template functions (include, toYaml). Add tests/README.md explaining test structure and how to run specific test suites.",
            "status": "done",
            "testStrategy": "Run 'helm unittest tests/unit/deployment_test.yaml' and verify all example tests pass. Run 'helm unittest --update-snapshot .' to generate initial snapshots. Modify template slightly and verify snapshot test detects change. Run 'helm unittest .' to execute all test files and verify complete test suite passes. Verify tests/README.md examples are accurate and executable.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create CONTRIBUTING.md and configure pre-commit hooks",
            "description": "Document TDD workflow in CONTRIBUTING.md and set up pre-commit hooks to enforce local test execution before commits",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Create CONTRIBUTING.md documenting TDD workflow: 1) Write test first - create test file in tests/unit/ defining expected behavior, 2) Implement template - create/modify template to satisfy test, 3) Run tests locally - execute 'helm unittest .' to verify test passes, 4) Run lint and policy checks - execute 'helm lint --strict' and 'conftest test <(helm template .)', 5) Commit both test and template together. Include sections on: development environment setup, running test suites, writing new tests, debugging test failures, CI/CD expectations. Create .pre-commit-config.yaml with hooks: trailing-whitespace, end-of-file-fixer, check-yaml, helm-unittest (custom hook running 'helm unittest .'), helm-lint (running 'helm lint --strict'). Install pre-commit framework and hooks with 'pre-commit install'. Add pre-commit installation instructions to CONTRIBUTING.md.",
            "status": "done",
            "testStrategy": "Verify CONTRIBUTING.md is comprehensive and all commands are accurate by following workflow step-by-step. Install pre-commit hooks with 'pre-commit install'. Make a test commit with failing test and verify pre-commit hook blocks commit. Make a test commit with passing tests and verify commit succeeds. Run 'pre-commit run --all-files' to verify all hooks execute correctly on entire repository.",
            "parentId": "undefined"
          }
        ],
        "complexity": 4,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break down the test infrastructure setup into: 1) helm-unittest installation and test directory structure creation, 2) Conftest/OPA setup with base security policies, 3) CI/CD pipeline configuration with GitHub Actions, 4) values.schema.json creation and validation setup, 5) example test file creation demonstrating different test patterns, 6) CONTRIBUTING.md and pre-commit hooks setup"
      },
      {
        "id": 2,
        "title": "Implement Core Kubernetes Resources with Memory Datastore",
        "description": "Create helper templates, ServiceAccount, RBAC, Secret, Service, and basic Deployment for memory-mode SpiceDB",
        "details": "Create templates/_helpers.tpl with functions: spicedb.name, spicedb.fullname, spicedb.chart, spicedb.labels, spicedb.selectorLabels, spicedb.serviceAccountName. Create templates/serviceaccount.yaml with configurable name, labels, annotations. Create templates/rbac.yaml with Role (get/list endpoints) and RoleBinding. Create templates/secret.yaml to generate preshared key (uses lookup + randAlphaNum 32 for idempotency). Create templates/service.yaml with ports: 50051 (grpc), 8443 (http), 9090 (metrics), 50053 (dispatch). Create templates/deployment.yaml with: image configuration, env vars for memory datastore, liveness probe (grpc:50051), readiness probe (grpc:50051), security context (runAsNonRoot: true, runAsUser: 1000, readOnlyRootFilesystem: true), volume mounts for secret. Create templates/NOTES.txt with connection instructions. Write comprehensive unit tests for each template (90%+ coverage). Create examples/dev-memory.yaml.",
        "testStrategy": "Unit tests for all helper functions verify correct output. ServiceAccount tests verify labels, annotations, naming. RBAC tests verify correct permissions and bindings. Service tests verify all ports configured with correct selectors. Deployment tests verify environment variables, probes, security context, image configuration. Integration test: helm template with default values renders valid manifests. Integration test: helm install on Minikube, pod reaches Ready state, grpc health check passes on port 50051. Snapshot tests for each resource capture expected output.",
        "priority": "high",
        "dependencies": [
          "1"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create _helpers.tpl with all helper functions and comprehensive tests",
            "description": "Implement templates/_helpers.tpl with all required helper functions: spicedb.name, spicedb.fullname, spicedb.chart, spicedb.labels, spicedb.selectorLabels, spicedb.serviceAccountName. Write comprehensive unit tests achieving 90%+ coverage.",
            "dependencies": [],
            "details": "Create templates/_helpers.tpl following Helm best practices. Implement spicedb.name (truncated to 63 chars), spicedb.fullname (release.name-chart.name truncated), spicedb.chart (chart.name-chart.version with + replaced by _), spicedb.labels (including helm.sh/chart, app.kubernetes.io/name, app.kubernetes.io/instance, app.kubernetes.io/version, app.kubernetes.io/managed-by), spicedb.selectorLabels (app.kubernetes.io/name and app.kubernetes.io/instance), spicedb.serviceAccountName (using values.serviceAccount.name or generated name). Write unit tests in tests/helpers_test.yaml using helm-unittest covering: name truncation edge cases, fullname override behavior, chart version formatting, label generation with various inputs, serviceAccountName logic (create vs. name), and edge cases for all functions.",
            "status": "done",
            "testStrategy": "Use helm unittest framework with tests/helpers_test.yaml. Test all helper functions with various value combinations, edge cases (long names, special characters), override scenarios, and nil values. Verify correct truncation, label formatting, and conditional logic. Achieve 90%+ code coverage.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement ServiceAccount template with tests",
            "description": "Create templates/serviceaccount.yaml with configurable name, labels, annotations, and conditional creation. Write comprehensive unit tests for all configuration scenarios.",
            "dependencies": [
              1
            ],
            "details": "Create templates/serviceaccount.yaml using serviceAccount.create conditional. Use spicedb.serviceAccountName helper for metadata.name. Apply spicedb.labels for metadata.labels. Support custom annotations via serviceAccount.annotations. Add values.yaml fields: serviceAccount.create (default true), serviceAccount.annotations (default {}), serviceAccount.name (default empty string, auto-generated if empty). Write unit tests in tests/serviceaccount_test.yaml covering: conditional creation based on serviceAccount.create, correct name generation from helper, label application, custom annotation handling, and interaction with serviceAccount.name override.",
            "status": "done",
            "testStrategy": "Unit tests verify ServiceAccount created only when serviceAccount.create is true. Test name generation using helper function. Verify labels match spicedb.labels output. Test annotation merging and custom annotations. Test serviceAccount.name override behavior. Use helm unittest with multiple test cases covering all value combinations.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement RBAC (Role/RoleBinding) with permission tests",
            "description": "Create templates/rbac.yaml with Role granting get/list permissions on endpoints and RoleBinding connecting to ServiceAccount. Write tests verifying correct permissions and bindings.",
            "dependencies": [
              2
            ],
            "details": "Create templates/rbac.yaml with conditional based on rbac.create. Define Role with apiGroups: [''], resources: ['endpoints'], verbs: ['get', 'list'] for service discovery. Create RoleBinding referencing the Role and ServiceAccount. Use spicedb.fullname for resource names and spicedb.labels for metadata. Add values.yaml: rbac.create (default true). Write unit tests in tests/rbac_test.yaml covering: conditional creation, correct API groups and resources, verb permissions (get/list only), RoleBinding subject references correct ServiceAccount, roleRef points to correct Role, and labels are properly applied.",
            "status": "done",
            "testStrategy": "Unit tests verify Role created with correct apiGroups, resources, and verbs. Test RoleBinding subject matches ServiceAccount name and namespace. Verify roleRef references the Role correctly. Test conditional creation based on rbac.create. Verify labels applied correctly. Use helm unittest with assertions on RBAC resource structure.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Secret template with preshared key generation and idempotency tests",
            "description": "Create templates/secret.yaml to generate SpiceDB preshared key using lookup function with randAlphaNum for idempotent secret generation. Write tests verifying idempotency and key format.",
            "dependencies": [
              1
            ],
            "details": "Create templates/secret.yaml for SpiceDB preshared key. Use lookup function to check for existing secret, if not found generate using randAlphaNum 32. Encode key using b64enc. Secret data key: preshared-key. Support secret.name override via values.yaml. Add annotations for Helm management. Implement idempotency pattern: {{- $secret := lookup \"v1\" \"Secret\" .Release.Namespace (include \"spicedb.fullname\" .) -}}{{- if $secret -}}use existing{{- else -}}generate new{{- end -}}. Write unit tests in tests/secret_test.yaml covering: secret creation with correct name, data field contains preshared-key, base64 encoding verification, and integration test for upgrade scenario (requires helm unittest with --with-subchart flag or manual verification).",
            "status": "done",
            "testStrategy": "Unit tests verify Secret created with correct name and type. Test data field contains preshared-key entry. Verify base64 encoding. Mock lookup function behavior for new and existing secrets. Integration test: install chart, capture secret value, upgrade chart, verify secret value unchanged (idempotency). Test secret.name override functionality.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Service template with multi-port configuration and tests",
            "description": "Create templates/service.yaml exposing four ports (grpc, http, metrics, dispatch) with proper naming, selectors, and annotations. Write comprehensive port configuration tests.",
            "dependencies": [
              1
            ],
            "details": "Create templates/service.yaml with type ClusterIP (configurable via service.type). Define four ports: grpc (50051, name: grpc, targetPort: grpc), http (8443, name: http, targetPort: http), metrics (9090, name: metrics, targetPort: metrics), dispatch (50053, name: dispatch, targetPort: dispatch). Use spicedb.selectorLabels for selector. Apply spicedb.labels to metadata. Support service.annotations, service.clusterIP. Add values.yaml: service.type (default ClusterIP), service.ports with overrides for each port, service.annotations. Write unit tests in tests/service_test.yaml covering: all four ports defined correctly, port names and numbers match specification, targetPort configuration, selector matches deployment labels, service type configuration, and custom annotations.",
            "status": "done",
            "testStrategy": "Unit tests verify Service contains exactly four ports with correct names, ports, and targetPorts. Test selector matches spicedb.selectorLabels output. Verify service type configurable. Test port overrides via values. Verify labels and annotations applied. Test with different service types (ClusterIP, LoadBalancer, NodePort). Use helm unittest with port array assertions.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement Deployment template with probes, security context, and environment variables",
            "description": "Create templates/deployment.yaml with SpiceDB container configuration including memory datastore environment variables, liveness/readiness probes, security hardening, and volume mounts for secrets.",
            "dependencies": [
              4,
              5
            ],
            "details": "Create templates/deployment.yaml with replicas from values (default 1). Container spec: image from values.image.repository, tag from values.image.tag (default Chart.appVersion), pullPolicy from values.image.pullPolicy. Environment variables: SPICEDB_DATASTORE_ENGINE=memory, SPICEDB_GRPC_PRESHARED_KEY from secret, SPICEDB_HTTP_ENABLED=true. Liveness probe: grpc port 50051, initialDelaySeconds: 5, periodSeconds: 10. Readiness probe: grpc port 50051, initialDelaySeconds: 3, periodSeconds: 5. Security context: runAsNonRoot: true, runAsUser: 1000, runAsGroup: 1000, fsGroup: 1000, readOnlyRootFilesystem: true, allowPrivilegeEscalation: false, capabilities.drop: [ALL]. Volume mount secret at /etc/spicedb/secrets. Define container ports: grpc (50051), http (8443), metrics (9090), dispatch (50053). Add resource requests/limits from values. Write unit tests in tests/deployment_test.yaml covering: image configuration, all environment variables, probe configuration, security context fields, volume mounts, port definitions, and resource specifications.",
            "status": "done",
            "testStrategy": "Unit tests verify Deployment container image uses values. Test environment variables include SPICEDB_DATASTORE_ENGINE=memory and preshared key from secret. Verify liveness and readiness probe configuration (grpc, delays, periods). Test security context fields (runAsNonRoot, runAsUser, readOnlyRootFilesystem, capabilities). Verify volume mount for secret. Test all four container ports defined. Verify resource requests/limits applied. Use helm unittest with deep object assertions.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Create NOTES.txt template",
            "description": "Implement templates/NOTES.txt providing connection instructions and usage guidance for accessing SpiceDB after installation.",
            "dependencies": [
              5
            ],
            "details": "Create templates/NOTES.txt displaying post-installation instructions. Include: success message, how to get preshared key from secret (kubectl get secret command), instructions for port-forwarding to access SpiceDB (kubectl port-forward for grpc 50051 and http 8443), example connection commands using zed CLI or grpcurl, link to SpiceDB documentation, and notes about current configuration (memory datastore, replica count). Use Helm template functions to display actual release name, namespace, and service name. Format output for readability in terminal. Write basic test in tests/notes_test.yaml verifying NOTES.txt renders without errors.",
            "status": "done",
            "testStrategy": "Manual verification: install chart and verify NOTES.txt displays correctly with accurate release-specific information. Unit test verifies template renders without errors. Test with different release names and namespaces to ensure dynamic values populate correctly. Verify kubectl commands use correct resource names.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Create examples/dev-memory.yaml and integration tests",
            "description": "Create example values file for development deployment using memory datastore and write comprehensive integration tests verifying all components work together.",
            "dependencies": [
              6,
              7
            ],
            "details": "Create examples/dev-memory.yaml with minimal development configuration: 1 replica, memory datastore, resource limits for development (cpu: 100m, memory: 128Mi requests), enabled monitoring annotations, development-friendly settings. Document all configurable options with comments. Write integration test suite in tests/integration_test.yaml using helm unittest covering: full chart rendering with dev-memory values, verify all resources created (ServiceAccount, RBAC, Secret, Service, Deployment), cross-resource references correct (Service selector matches Deployment labels, RoleBinding references ServiceAccount, Deployment mounts Secret), label consistency across all resources, and end-to-end scenario test. Create tests/test-values directory with additional test value files for edge cases.",
            "status": "done",
            "testStrategy": "Integration tests verify complete chart installation with examples/dev-memory.yaml. Test all resources created successfully. Verify cross-resource consistency: Service selectors match Deployment, Secret referenced in Deployment, ServiceAccount used in RBAC. Test label propagation across all resources. Create additional test scenarios: minimal values, maximal values, disabled components. Use helm unittest with full chart rendering. Manual integration test: helm install with dev-memory.yaml, verify all pods running, test connectivity to grpc and http ports.",
            "parentId": "undefined"
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 8,
        "expansionPrompt": "Divide into: 1) Create _helpers.tpl with all helper functions and comprehensive tests, 2) Implement ServiceAccount template with tests, 3) Implement RBAC (Role/RoleBinding) with permission tests, 4) Implement Secret template with preshared key generation and idempotency tests, 5) Implement Service template with multi-port configuration and tests, 6) Implement Deployment template with probes, security context, and environment variables, 7) Create NOTES.txt template, 8) Create examples/dev-memory.yaml and integration tests"
      },
      {
        "id": 3,
        "title": "Implement PostgreSQL and CockroachDB Datastore Support",
        "description": "Add datastore configuration logic, connection string generation, and external secret support for PostgreSQL and CockroachDB",
        "details": "Extend values.yaml with config.datastoreEngine enum (postgres, cockroachdb, memory), config.datastoreURI for explicit URI, existingSecret for external secret reference. Update templates/secret.yaml to conditionally create secret only when existingSecret is empty. Add datastore-specific environment variables to templates/deployment.yaml: SPICEDB_DATASTORE_ENGINE, SPICEDB_DATASTORE_CONN_URI (from secret). Implement connection string generation logic in _helpers.tpl: postgres format (postgresql://user:pass@host:port/db?sslmode=disable), cockroachdb format (postgresql://user:pass@host:port/db?sslmode=verify-full). Add values.yaml fields for datastore configuration: hostname, port, username, password, database, sslMode, sslRootCert, sslCert, sslKey. Create examples/production-postgres.yaml with PostgreSQL configuration. Create examples/production-cockroachdb.yaml with CockroachDB configuration. Create examples/postgres-external-secrets.yaml demonstrating External Secrets Operator pattern (ExternalSecret creates Secret, chart references via existingSecret). Document External Secrets Operator integration pattern.",
        "testStrategy": "Deployment tests verify correct environment variables for PostgreSQL engine. Deployment tests verify correct environment variables for CockroachDB engine. Connection string generation tests verify correct format for each datastore. Secret mounting tests verify both inline secrets and existingSecret pattern. Snapshot tests for PostgreSQL configuration. Snapshot tests for CockroachDB configuration. Integration tests: deploy to Minikube with PostgreSQL container, verify SpiceDB connects successfully. Integration tests: deploy with CockroachDB container, verify connection. External Secrets Operator compatibility test: verify values structure works with ESO.",
        "priority": "high",
        "dependencies": [
          "2"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend values.yaml with datastore configuration fields and validation",
            "description": "Add all necessary datastore configuration fields to values.yaml including datastoreEngine enum, connection parameters, SSL/TLS settings, and existingSecret reference with appropriate defaults and documentation",
            "dependencies": [],
            "details": "Add config.datastoreEngine with enum values (postgres, cockroachdb, memory) defaulting to memory. Add config.datastoreURI for explicit connection string override. Add config.existingSecret for external secret reference. Add datastore connection fields: hostname (default: localhost), port (postgres: 5432, cockroachdb: 26257), username (default: spicedb), password, database (default: spicedb), sslMode (postgres: disable, cockroachdb: verify-full), sslRootCert, sslCert, sslKey. Include inline comments documenting each field's purpose and valid values. Group related fields logically under config.datastore section.",
            "status": "done",
            "testStrategy": "Helm lint validates values.yaml schema. Template rendering tests verify default values produce valid output. Validation tests for invalid datastoreEngine values. Tests verify schema allows all documented SSL/TLS parameters.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement connection string generation logic in _helpers.tpl for PostgreSQL and CockroachDB",
            "description": "Create helper template functions to generate properly formatted connection strings for PostgreSQL and CockroachDB datastores with SSL/TLS parameter handling and credential interpolation",
            "dependencies": [
              1
            ],
            "details": "Implement {{- define \"spicedb.datastoreConnectionString\" -}} helper in templates/_helpers.tpl. Generate PostgreSQL format: postgresql://{{username}}:{{password}}@{{hostname}}:{{port}}/{{database}}?sslmode={{sslMode}} with conditional SSL cert parameters (sslrootcert, sslcert, sslkey) when provided. Generate CockroachDB format with same structure but default sslmode=verify-full. Support config.datastoreURI override to bypass generation. Handle URL encoding of special characters in credentials. Add conditional logic to only generate when datastoreEngine is not memory.",
            "status": "done",
            "testStrategy": "Unit tests via helm unittest verify PostgreSQL connection string format with minimal parameters. Tests verify CockroachDB connection string with SSL parameters included. Tests verify special characters in passwords are URL-encoded. Tests verify datastoreURI override bypasses generation. Tests verify memory engine produces no connection string.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Update templates/secret.yaml for conditional creation and existingSecret support",
            "description": "Modify secret template to conditionally create Secret resource only when existingSecret is not specified, storing generated connection string and credentials",
            "dependencies": [
              2
            ],
            "details": "Wrap entire templates/secret.yaml in {{- if not .Values.config.existingSecret }} conditional. Store base64-encoded datastore connection string under key datastore-uri using helper function. Include individual credential fields (username, password) for potential separate access. Add SSL certificate data (sslRootCert, sslCert, sslKey) when provided. Set secret type to Opaque. Include standard labels and annotations from _helpers.tpl. Add documentation comment explaining existingSecret usage pattern.",
            "status": "done",
            "testStrategy": "Template rendering tests verify Secret created when existingSecret is empty. Tests verify Secret not created when existingSecret is set. Tests verify datastore-uri key contains properly formatted connection string. Tests verify SSL certificate fields included when configured. Tests verify Secret contains correct labels and name.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Update templates/deployment.yaml with datastore environment variables",
            "description": "Add datastore-specific environment variables to the SpiceDB deployment container, sourcing connection string from either generated or existing Secret",
            "dependencies": [
              3
            ],
            "details": "Add SPICEDB_DATASTORE_ENGINE environment variable with value from .Values.config.datastoreEngine. Add SPICEDB_DATASTORE_CONN_URI environment variable sourced from secret key datastore-uri, using either generated secret name or .Values.config.existingSecret. Use valueFrom.secretKeyRef with name: {{ .Values.config.existingSecret | default (include \"spicedb.fullname\" .) }}-secret and key: datastore-uri. Ensure environment variables are set only when datastoreEngine is not memory. Position after existing environment variables in deployment spec.",
            "status": "done",
            "testStrategy": "Deployment template tests verify SPICEDB_DATASTORE_ENGINE set with correct value. Tests verify SPICEDB_DATASTORE_CONN_URI references correct secret name (generated). Tests verify SPICEDB_DATASTORE_CONN_URI references existingSecret when configured. Tests verify environment variables not set when datastoreEngine is memory. Integration test verifies deployed pod receives correct environment variables.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create examples/production-postgres.yaml with comprehensive configuration and tests",
            "description": "Create example values file demonstrating production PostgreSQL deployment with connection pooling, SSL/TLS, resource limits, and high availability settings",
            "dependencies": [
              4
            ],
            "details": "Create examples/production-postgres.yaml with config.datastoreEngine: postgres. Configure connection parameters: hostname: postgres.database.svc.cluster.local, port: 5432, username: spicedb, database: spicedb_production, sslMode: require, sslRootCert path. Set replicas: 3 for HA. Configure resources.requests (cpu: 500m, memory: 512Mi) and limits (cpu: 2, memory: 2Gi). Enable podDisruptionBudget and autoscaling. Add monitoring.enabled: true and serviceMonitor.enabled: true. Include inline comments explaining production considerations.",
            "status": "done",
            "testStrategy": "Helm template rendering with production-postgres.yaml produces valid manifests. Tests verify PostgreSQL-specific connection string format in generated Secret. Tests verify deployment has 3 replicas and correct resource limits. Tests verify PDB and HPA created. Integration test with PostgreSQL database verifies successful connection and schema migration.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create examples/production-cockroachdb.yaml with configuration and tests",
            "description": "Create example values file demonstrating production CockroachDB deployment with SSL certificate authentication and distributed deployment settings",
            "dependencies": [
              4
            ],
            "details": "Create examples/production-cockroachdb.yaml with config.datastoreEngine: cockroachdb. Configure connection parameters: hostname: cockroachdb-public.database.svc.cluster.local, port: 26257, username: spicedb, database: spicedb, sslMode: verify-full. Configure SSL certificates: sslRootCert, sslCert, sslKey (with paths). Set replicas: 5 for distributed consistency. Configure resources matching CockroachDB workload patterns. Enable dispatch.enabled: true for distributed cluster mode. Add topologySpreadConstraints for zone distribution. Include production tuning parameters and comments.",
            "status": "done",
            "testStrategy": "Helm template rendering with production-cockroachdb.yaml produces valid manifests. Tests verify CockroachDB connection string includes sslmode=verify-full. Tests verify SSL certificate mounting configuration. Tests verify dispatch cluster mode enabled. Tests verify topology spread constraints configured. Integration test with CockroachDB cluster verifies connection with mTLS.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Create examples/postgres-external-secrets.yaml and document External Secrets Operator integration",
            "description": "Create example demonstrating External Secrets Operator integration pattern with ExternalSecret resource creating Secret consumed by Helm chart via existingSecret, and add comprehensive documentation",
            "dependencies": [
              4
            ],
            "details": "Create examples/postgres-external-secrets.yaml with two sections: ExternalSecret resource (apiVersion: external-secrets.io/v1beta1) fetching credentials from AWS Secrets Manager/GCP Secret Manager/Vault, targeting secret name spicedb-datastore-secret. Include dataFrom or data entries for datastore-uri, username, password keys. Second section shows Helm values with config.existingSecret: spicedb-datastore-secret, config.datastoreEngine: postgres. Add comprehensive comments explaining: External Secrets Operator installation, SecretStore/ClusterSecretStore configuration, ExternalSecret creation pattern, Helm chart integration via existingSecret. Document credential rotation and security best practices.",
            "status": "done",
            "testStrategy": "YAML validation of ExternalSecret resource structure. Documentation review for completeness and clarity. Manual integration test with External Secrets Operator and AWS Secrets Manager backend. Tests verify Helm chart correctly consumes externally-created Secret. Tests verify no duplicate Secret creation when existingSecret is set.",
            "parentId": "undefined"
          }
        ],
        "complexity": 6,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Break into: 1) Extend values.yaml with datastore configuration fields and validation, 2) Implement connection string generation logic in _helpers.tpl for PostgreSQL and CockroachDB with tests, 3) Update secret.yaml for conditional creation and existingSecret support, 4) Update deployment.yaml with datastore environment variables, 5) Create examples/production-postgres.yaml with comprehensive tests, 6) Create examples/production-cockroachdb.yaml with tests, 7) Create examples/postgres-external-secrets.yaml and document External Secrets Operator integration"
      },
      {
        "id": 4,
        "title": "Implement Automated Database Migration System",
        "description": "Create Helm hooks for pre-install/pre-upgrade migrations with job templates, cleanup hooks, and phased migration support",
        "details": "Create templates/hooks/migration-job.yaml with annotations: helm.sh/hook: pre-install,pre-upgrade, helm.sh/hook-weight: \"0\", helm.sh/hook-delete-policy: before-hook-creation. Migration job spec: restartPolicy: OnFailure, backoffLimit: 3, activeDeadlineSeconds: 600. Container: same image as deployment, command: [spicedb, migrate, head], environment variables from deployment (datastore connection). Add values.yaml fields: migrations.enabled (default true), migrations.logLevel, migrations.targetMigration, migrations.targetPhase. Implement target migration support: override command to [spicedb, migrate, <target>] when targetMigration set. Implement phased migration: command [spicedb, migrate, --phase=<phase>] when targetPhase set. Create templates/hooks/migration-cleanup.yaml with annotations: helm.sh/hook: post-install,post-upgrade, hook-delete-policy: hook-succeeded. Cleanup job uses kubectl delete jobs with label selectors. Add migration hash tracking via deployment annotations (checksum/migration-config) to trigger restarts. Document migration troubleshooting: view logs, manual rollback, dry-run support.",
        "testStrategy": "Migration job creation tests verify correct annotations and hook weights. Hook annotation tests verify pre-install and pre-upgrade hooks present. Environment variable tests verify migration jobs receive datastore configuration. PostgreSQL migration test: install chart, verify migration job runs to completion, verify deployment starts after migration. CockroachDB migration test: same as PostgreSQL. Target migration tests: set targetMigration, verify job command includes target version. Phased migration tests: set targetPhase, verify job command includes phase flag. Cleanup hook tests: verify cleanup job created, verify old migration jobs deleted. Upgrade test: helm upgrade with new version, verify migration runs before deployment update. Failure handling test: break migration job, verify deployment doesn't start, verify helm install fails.",
        "priority": "high",
        "dependencies": [
          "3"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create migration-job.yaml template with Helm hook annotations",
            "description": "Create templates/hooks/migration-job.yaml with proper Helm hook annotations for pre-install and pre-upgrade lifecycle phases, including hook weights and delete policies",
            "dependencies": [],
            "details": "Create templates/hooks/migration-job.yaml with annotations: helm.sh/hook: pre-install,pre-upgrade, helm.sh/hook-weight: \"0\", helm.sh/hook-delete-policy: before-hook-creation. Add conditional rendering based on migrations.enabled value. Include proper metadata labels matching the chart. Ensure hook weight is set to run before other hooks if needed.",
            "status": "done",
            "testStrategy": "Verify hook annotations are present and correct. Test conditional rendering when migrations.enabled is false. Validate hook weight ordering. Test hook-delete-policy ensures old migration jobs are cleaned up before new ones run.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement migration job spec with retry logic and timeouts",
            "description": "Define the Job spec with restartPolicy, backoffLimit, activeDeadlineSeconds, container configuration, and environment variable inheritance from deployment",
            "dependencies": [
              1
            ],
            "details": "Implement job spec with restartPolicy: OnFailure, backoffLimit: 3, activeDeadlineSeconds: 600. Container uses same image as deployment with command: [spicedb, migrate, head]. Inherit environment variables from deployment for datastore connection (SPICEDB_DATASTORE_ENGINE, connection strings, credentials). Add support for migrations.logLevel configuration. Include resource requests/limits if specified in values.",
            "status": "done",
            "testStrategy": "Test job spec has correct restartPolicy and backoff settings. Verify activeDeadlineSeconds is configurable. Test environment variable inheritance from deployment ensures migration job has database connectivity. Test with PostgreSQL and CockroachDB to verify successful migrations. Simulate failure scenarios to verify retry logic.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Add target migration and phased migration configuration support",
            "description": "Extend values.yaml with migration configuration fields and implement command overrides for target migration and phased migration scenarios",
            "dependencies": [
              2
            ],
            "details": "Add values.yaml fields: migrations.enabled (default true), migrations.logLevel, migrations.targetMigration, migrations.targetPhase. Override command to [spicedb, migrate, <target>] when targetMigration is set. Override command to [spicedb, migrate, --phase=<phase>] when targetPhase is set. Ensure proper precedence when both are specified. Document valid phase values (write, read, complete).",
            "status": "done",
            "testStrategy": "Test default command runs 'migrate head'. Test targetMigration overrides command correctly with specific migration target. Test targetPhase overrides command with phase flag. Test validation when both targetMigration and targetPhase are set. Verify configuration in values.yaml schema.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create migration-cleanup.yaml hook for post-migration cleanup",
            "description": "Implement post-install and post-upgrade cleanup hook to remove completed migration jobs using kubectl delete with label selectors",
            "dependencies": [
              2
            ],
            "details": "Create templates/hooks/migration-cleanup.yaml with annotations: helm.sh/hook: post-install,post-upgrade, helm.sh/hook-delete-policy: hook-succeeded. Cleanup job uses kubectl image with command to delete jobs using label selectors (app.kubernetes.io/name, app.kubernetes.io/instance). Include proper RBAC permissions for job deletion in cleanup ServiceAccount. Set activeDeadlineSeconds for cleanup job to prevent hanging.",
            "status": "done",
            "testStrategy": "Verify cleanup hook has correct post-install and post-upgrade annotations. Test cleanup job successfully deletes migration jobs after successful completion. Verify hook-succeeded delete policy removes cleanup job itself. Test RBAC permissions allow job deletion. Test cleanup doesn't run if migration fails.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement migration hash tracking via deployment annotations",
            "description": "Add checksum annotation to deployment template that tracks migration configuration changes to trigger pod restarts when migrations change",
            "dependencies": [
              3
            ],
            "details": "Add annotation to templates/deployment.yaml: checksum/migration-config: {{ include (print $.Template.BasePath \"/hooks/migration-job.yaml\") . | sha256sum }}. This ensures deployment pods restart when migration configuration changes. Include migrations.targetMigration, migrations.targetPhase, and other migration-related values in the checksum calculation. Document behavior in NOTES.txt.\n<info added on 2025-11-08T04:55:52.049Z>\nI'll analyze the codebase to understand the implementation before generating the update.Successfully implemented in charts/spicedb/templates/deployment.yaml at line 17 using pattern {{ .Values.migrations | toJson | sha256sum }}. The annotation is correctly placed in the pod template metadata (spec.template.metadata.annotations), not the Deployment metadata, ensuring proper rolling update triggers. All migrations configuration fields from values.yaml lines 109-150 are included in the checksum: enabled, logLevel, targetMigration, targetPhase, resources, and cleanup. Verified through testing that changes to any migration configuration value produce unique checksums, confirming the implementation correctly triggers pod restarts when migration settings change. The annotation merges properly with user-defined podAnnotations via the template logic at lines 18-20.\n</info added on 2025-11-08T04:55:52.049Z>",
            "status": "done",
            "testStrategy": "Test deployment annotation includes migration configuration checksum. Verify changing migrations.targetMigration triggers deployment rollout. Verify changing migrations.targetPhase triggers deployment rollout. Test that unrelated configuration changes don't affect migration checksum. Validate checksum calculation includes all relevant migration fields.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Write comprehensive tests for migration scenarios",
            "description": "Create helm-unittest test suite covering success, failure, upgrade, rollback, and various configuration scenarios for the migration system",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Create tests/hooks/migration-job_test.yaml with test cases: migration job creation with correct annotations, hook weights, environment variables, retry configuration, target migration override, phased migration override. Create tests for cleanup hook. Test migration with PostgreSQL and CockroachDB datastores. Test upgrade scenarios where migration changes. Test failure scenarios with backoffLimit. Test migration disabled scenario. Test checksum annotation changes trigger restarts.",
            "status": "done",
            "testStrategy": "Run helm unittest to verify all test cases pass. Test install scenario creates migration job before deployment. Test upgrade scenario runs migration job before deployment update. Test failure scenario respects backoffLimit and activeDeadlineSeconds. Test cleanup removes old jobs. Test with both PostgreSQL and CockroachDB configurations. Verify disabled migrations skip job creation.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Create migration troubleshooting documentation",
            "description": "Document migration procedures including viewing logs, manual rollback steps, dry-run support, common issues, and debugging techniques",
            "dependencies": [
              6
            ],
            "details": "Add documentation section covering: viewing migration job logs (kubectl logs job/spicedb-migration), checking migration job status (kubectl get jobs), manual rollback procedures (helm rollback), dry-run migration testing (helm install --dry-run), common failure scenarios (timeout, connection issues, schema conflicts), debugging techniques (describe job, check events), targetMigration and targetPhase usage examples. Include in chart NOTES.txt and README.md with examples.\n<info added on 2025-11-08T04:56:26.301Z>\nComplete implementation verified. README.md migration documentation comprehensively covers all required areas: log viewing with component label selector, job status checking with label selector, manual rollback with schema downgrade warnings, dry-run testing approaches, all common failure scenarios (timeout with activeDeadlineSeconds extension example, connection issues with secret verification steps, job stuck with deletion/retry procedure), debugging techniques with kubectl logs/describe/get events examples, phased migration workflow with complete 4-step process including kubectl wait verification commands, targetMigration and targetPhase parameter usage in all relevant examples. Documentation structure: overview section explaining automatic behavior, configuration table with all migrations.* parameters, common operations section with standard and manual migration procedures, dedicated phased migration section with zero-downtime workflow, troubleshooting section with diagnosis and resolution steps for each failure scenario, examples section with 5 real-world use cases. All kubectl/helm commands tested and copy-paste ready. Documentation integrated into existing README.md following established formatting patterns. Subtask completed successfully.\n</info added on 2025-11-08T04:56:26.301Z>",
            "status": "done",
            "testStrategy": "Review documentation for completeness and accuracy. Verify all kubectl commands are correct. Test dry-run examples produce expected output. Ensure rollback procedures are safe and tested. Validate troubleshooting steps cover common failure modes discovered during testing.",
            "updatedAt": "2025-11-08T04:56:35.882Z",
            "parentId": "undefined"
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Divide into: 1) Create migration-job.yaml template with proper Helm hook annotations and weights, 2) Implement job spec with retry logic, timeouts, and resource constraints, 3) Add support for target migration and phased migration configurations, 4) Create migration-cleanup.yaml hook for post-migration cleanup, 5) Implement migration hash tracking via deployment annotations to trigger restarts, 6) Write comprehensive tests for all migration scenarios (success, failure, upgrade, rollback), 7) Create migration troubleshooting documentation",
        "updatedAt": "2025-11-08T04:56:35.882Z"
      },
      {
        "id": 5,
        "title": "Implement Comprehensive TLS Support for All Endpoints",
        "description": "Add TLS configuration for gRPC, HTTP gateway, dispatch cluster, and datastore connections with cert-manager integration",
        "details": "Extend values.yaml with tls.enabled, tls.grpc (secretName, certPath, keyPath, caPath), tls.http (same structure), tls.dispatch (secretName, certPath, keyPath, caPath for mTLS), tls.datastore (secretName, caPath). Update templates/deployment.yaml to mount TLS secrets as volumes when enabled. Add TLS environment variables: SPICEDB_GRPC_TLS_CERT_PATH, SPICEDB_GRPC_TLS_KEY_PATH, SPICEDB_GRPC_TLS_CA_PATH for gRPC. Add SPICEDB_HTTP_TLS_CERT_PATH, SPICEDB_HTTP_TLS_KEY_PATH for HTTP gateway. Add SPICEDB_DISPATCH_CLUSTER_TLS_CERT_PATH, SPICEDB_DISPATCH_CLUSTER_TLS_KEY_PATH, SPICEDB_DISPATCH_CLUSTER_TLS_CA_PATH for dispatch mTLS. Update datastore connection string to include sslmode=verify-full, sslrootcert, sslcert, sslkey when tls.datastore enabled. Update readiness/liveness probes to use HTTPS scheme when TLS enabled. Harden security context: capabilities.drop [ALL], allowPrivilegeEscalation: false, seccompProfile.type: RuntimeDefault. Create examples/production-cockroachdb-tls.yaml with full TLS configuration. Create examples/cert-manager-integration.yaml demonstrating Certificate CRD usage. Document cert-manager integration: Certificate resource, Issuer/ClusterIssuer, automatic renewal.",
        "testStrategy": "TLS secret mounting tests verify volumes created for each TLS type. TLS environment variable tests verify correct paths for gRPC, HTTP, dispatch, datastore. Multi-TLS endpoint tests: enable all TLS types, verify all environment variables and volumes present. Security context tests verify non-root user, read-only filesystem, dropped capabilities. Certificate path configuration tests verify custom paths override defaults. Snapshot tests for full TLS configuration. Integration test: deploy with self-signed certificates, verify TLS connections work. Probe tests: verify readiness/liveness probes use HTTPS when TLS enabled. OPA policy tests: verify deployment passes Pod Security Standards restricted profile. cert-manager integration test: deploy with Certificate CRD, verify secret created and mounted.",
        "priority": "high",
        "dependencies": [
          "4"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend values.yaml with TLS configuration structure for all endpoints",
            "description": "Add comprehensive TLS configuration options to values.yaml for gRPC, HTTP gateway, dispatch cluster, and datastore endpoints with certificate path configurations",
            "dependencies": [],
            "details": "Add tls.enabled (global toggle), tls.grpc object with secretName, certPath, keyPath, caPath fields. Add tls.http object with secretName, certPath, keyPath fields. Add tls.dispatch object with secretName, certPath, keyPath, caPath for mTLS support. Add tls.datastore object with secretName, caPath for database connection encryption. Include sensible defaults for certificate paths (/etc/spicedb/tls/) and document each field with inline comments explaining purpose and usage.\n<info added on 2025-11-08T05:03:23.311Z>\nConfiguration successfully added with 120 lines of comprehensive TLS structure in values.yaml (lines 108-227). Implemented global toggle (tls.enabled), gRPC server TLS (secretName, certPath, keyPath, caPath), HTTP server TLS (secretName, certPath, keyPath), dispatch cluster mTLS (secretName, certPath, keyPath, caPath), and datastore client TLS (secretName, caPath). All paths use /etc/spicedb/tls/ prefix with endpoint-specific subdirectories. Included inline documentation explaining purpose, secret structure requirements, cert-manager integration examples, and mTLS vs server TLS differences. Configuration maintains existing values.yaml 2-space indentation style and is positioned between config.datastore (line 106) and migrations (line 229) sections.\n</info added on 2025-11-08T05:03:23.311Z>",
            "status": "done",
            "testStrategy": "Unit tests verify TLS configuration schema renders correctly in values.yaml. Test default values are sensible and paths follow /etc/spicedb/tls/ convention. Validate all four TLS endpoint configurations (grpc, http, dispatch, datastore) have required fields."
          },
          {
            "id": 2,
            "title": "Implement TLS secret mounting logic in deployment.yaml",
            "description": "Add conditional volume and volumeMount configurations to deployment.yaml template for mounting TLS secrets when TLS is enabled for any endpoint",
            "dependencies": [
              1
            ],
            "details": "Add volumes section with conditional blocks using {{ if .Values.tls.grpc.secretName }}, {{ if .Values.tls.http.secretName }}, {{ if .Values.tls.dispatch.secretName }}, {{ if .Values.tls.datastore.secretName }}. Each volume references the corresponding secret and mounts to /etc/spicedb/tls/{grpc,http,dispatch,datastore}. Add volumeMounts in container spec pointing to these paths. Ensure volume names are unique (e.g., tls-grpc-certs, tls-http-certs, tls-dispatch-certs, tls-datastore-certs) and mount paths align with certificate path values.",
            "status": "done",
            "testStrategy": "Test volume creation when each TLS type enabled independently. Test multiple TLS types enabled simultaneously creates all volumes. Verify volumeMount paths match certificate path configuration in values.yaml. Test volumes not created when TLS disabled."
          },
          {
            "id": 3,
            "title": "Add gRPC TLS environment variables and configuration",
            "description": "Configure gRPC endpoint TLS by adding environment variables for certificate paths and enabling TLS in SpiceDB gRPC server configuration",
            "dependencies": [
              2
            ],
            "details": "Add conditional environment variables in deployment.yaml when .Values.tls.grpc.secretName is set: SPICEDB_GRPC_TLS_CERT_PATH pointing to {{ .Values.tls.grpc.certPath }}, SPICEDB_GRPC_TLS_KEY_PATH pointing to {{ .Values.tls.grpc.keyPath }}, SPICEDB_GRPC_TLS_CA_PATH pointing to {{ .Values.tls.grpc.caPath }}. Ensure paths reference mounted volume locations. Set default paths to /etc/spicedb/tls/grpc/tls.crt, /etc/spicedb/tls/grpc/tls.key, /etc/spicedb/tls/grpc/ca.crt in values.yaml.",
            "status": "done",
            "testStrategy": "Verify SPICEDB_GRPC_TLS_* environment variables present when tls.grpc.secretName configured. Test environment variables point to correct mounted paths. Verify variables not present when gRPC TLS disabled. Test certificate path customization via values.yaml."
          },
          {
            "id": 4,
            "title": "Add HTTP gateway TLS environment variables and configuration",
            "description": "Configure HTTP gateway endpoint TLS by adding environment variables for certificate and key paths",
            "dependencies": [
              2
            ],
            "details": "Add conditional environment variables in deployment.yaml when .Values.tls.http.secretName is set: SPICEDB_HTTP_TLS_CERT_PATH pointing to {{ .Values.tls.http.certPath }}, SPICEDB_HTTP_TLS_KEY_PATH pointing to {{ .Values.tls.http.keyPath }}. Note that HTTP gateway typically does not require CA certificate for server TLS. Set default paths to /etc/spicedb/tls/http/tls.crt and /etc/spicedb/tls/http/tls.key in values.yaml.",
            "status": "done",
            "testStrategy": "Verify SPICEDB_HTTP_TLS_CERT_PATH and SPICEDB_HTTP_TLS_KEY_PATH present when tls.http.secretName configured. Test paths point to mounted volume locations. Verify variables not present when HTTP TLS disabled. Test with custom certificate paths."
          },
          {
            "id": 5,
            "title": "Add dispatch cluster mTLS configuration with CA certificate support",
            "description": "Configure mutual TLS for dispatch cluster communication with certificate, key, and CA certificate paths for inter-pod authentication",
            "dependencies": [
              2
            ],
            "details": "Add conditional environment variables in deployment.yaml when .Values.tls.dispatch.secretName is set: SPICEDB_DISPATCH_CLUSTER_TLS_CERT_PATH pointing to {{ .Values.tls.dispatch.certPath }}, SPICEDB_DISPATCH_CLUSTER_TLS_KEY_PATH pointing to {{ .Values.tls.dispatch.keyPath }}, SPICEDB_DISPATCH_CLUSTER_TLS_CA_PATH pointing to {{ .Values.tls.dispatch.caPath }}. Dispatch requires mTLS for secure inter-pod communication, so all three certificate types (cert, key, CA) are mandatory. Set default paths to /etc/spicedb/tls/dispatch/ directory.",
            "status": "done",
            "testStrategy": "Verify all three dispatch TLS environment variables present when tls.dispatch.secretName configured. Test mTLS configuration with CA certificate mounted correctly. Verify variables point to correct paths for mutual authentication. Test dispatch TLS independently from other TLS endpoints."
          },
          {
            "id": 6,
            "title": "Update datastore connection strings for TLS support",
            "description": "Modify PostgreSQL and CockroachDB connection string generation to include SSL parameters when datastore TLS is enabled",
            "dependencies": [
              2
            ],
            "details": "Update SPICEDB_DATASTORE_CONN_URI environment variable generation in deployment.yaml to append SSL parameters when .Values.tls.datastore.secretName is set. For PostgreSQL: append ?sslmode=verify-full&sslrootcert={{ .Values.tls.datastore.caPath }}. For CockroachDB: append ?sslmode=verify-full&sslrootcert={{ .Values.tls.datastore.caPath }}. If client certificates required, add &sslcert={{ .Values.tls.datastore.certPath }}&sslkey={{ .Values.tls.datastore.keyPath }}. Handle connection string templating to avoid duplicate parameters.",
            "status": "done",
            "testStrategy": "Test PostgreSQL connection string includes sslmode=verify-full when datastore TLS enabled. Test CockroachDB connection string includes SSL parameters. Verify sslrootcert path points to mounted CA certificate. Test connection string without TLS parameters when disabled. Test client certificate parameters when provided."
          },
          {
            "id": 7,
            "title": "Update probes to use HTTPS and harden security context",
            "description": "Modify readiness and liveness probes to use HTTPS scheme when TLS enabled and implement Pod Security Standards restricted profile requirements",
            "dependencies": [
              3,
              4
            ],
            "details": "Update httpGet probes in deployment.yaml to conditionally use scheme: HTTPS when .Values.tls.http.secretName or .Values.tls.grpc.secretName is set (probes target HTTP gateway). Add securityContext to container spec: capabilities.drop [ALL], allowPrivilegeEscalation: false, readOnlyRootFilesystem: true (if supported), runAsNonRoot: true, seccompProfile.type: RuntimeDefault. Add pod-level securityContext: fsGroup: 1000, runAsUser: 1000, runAsGroup: 1000 for non-root execution.",
            "status": "done",
            "testStrategy": "Verify probes use HTTPS scheme when TLS enabled. Test probes use HTTP when TLS disabled. Verify security context drops all capabilities. Test allowPrivilegeEscalation is false. Verify seccomp profile set to RuntimeDefault. Test pod runs as non-root user."
          },
          {
            "id": 8,
            "title": "Create production TLS examples and cert-manager integration documentation",
            "description": "Create example configurations demonstrating production TLS setup with CockroachDB and cert-manager integration for automated certificate management",
            "dependencies": [
              1,
              3,
              4,
              5,
              6,
              7
            ],
            "details": "Create examples/production-cockroachdb-tls.yaml showing complete TLS configuration for all endpoints (gRPC, HTTP, dispatch, datastore) with CockroachDB backend using certificate secrets. Create examples/cert-manager-integration.yaml demonstrating Certificate CRD usage with cert-manager.io/v1 API, showing Issuer/ClusterIssuer configuration, Certificate resources for each endpoint, and automatic secret creation. Document in values.yaml comments: cert-manager integration steps, Certificate resource format, automatic renewal behavior, and recommended Issuer configuration for production.",
            "status": "done",
            "testStrategy": "Validate production-cockroachdb-tls.yaml renders correctly with helm template. Verify cert-manager-integration.yaml includes valid Certificate CRDs. Test example configurations are syntactically correct. Verify documentation covers all TLS endpoints and cert-manager integration workflow."
          }
        ],
        "complexity": 8,
        "recommendedSubtasks": 8,
        "expansionPrompt": "Break into: 1) Extend values.yaml with TLS configuration structure for all endpoints (gRPC, HTTP, dispatch, datastore), 2) Implement TLS secret mounting logic in deployment.yaml with volume and volumeMount configurations, 3) Add gRPC TLS environment variables and configuration, 4) Add HTTP gateway TLS environment variables and configuration, 5) Add dispatch cluster mTLS configuration with CA certificate support, 6) Update datastore connection strings for TLS (sslmode, certificates), 7) Update probes to use HTTPS scheme when TLS enabled and harden security context, 8) Create examples (production-cockroachdb-tls.yaml, cert-manager-integration.yaml) and cert-manager integration documentation"
      },
      {
        "id": 6,
        "title": "Implement High Availability Configuration",
        "description": "Add PodDisruptionBudget, HorizontalPodAutoscaler, anti-affinity rules, topology spread constraints, and rolling update strategy",
        "details": "Create templates/poddisruptionbudget.yaml with maxUnavailable: 1, selector matching deployment labels, enabled via podDisruptionBudget.enabled (default true when replicas > 1). Create templates/hpa.yaml with minReplicas, maxReplicas, targetCPUUtilizationPercentage (80), targetMemoryUtilizationPercentage (80), enabled via autoscaling.enabled. Update templates/deployment.yaml with pod anti-affinity: preferredDuringSchedulingIgnoredDuringExecution, weight 100, topologyKey: topology.kubernetes.io/zone, labelSelector matching app.kubernetes.io/name. Add topologySpreadConstraints: maxSkew 1, topologyKey: topology.kubernetes.io/zone, whenUnsatisfiable: ScheduleAnyway. Update deployment strategy: RollingUpdate with maxUnavailable: 0, maxSurge: 1 for zero downtime. Add resources.requests (cpu: 500m, memory: 1Gi) and resources.limits (cpu: 2000m, memory: 4Gi) with configurable values. Extend values.yaml with: replicas (default 3), autoscaling.minReplicas (default 2), autoscaling.maxReplicas (default 10), podDisruptionBudget.maxUnavailable (default 1). Create examples/production-ha.yaml with full HA configuration.",
        "testStrategy": "PDB tests verify maxUnavailable set correctly, selector matches deployment. HPA tests verify min/max replicas, metric targets, selector matches deployment. Affinity tests verify anti-affinity rules present, correct topology key and label selector. Topology spread tests verify constraints configured correctly. Resource limit tests verify requests and limits set on containers. Rolling update tests verify maxUnavailable: 0 and maxSurge: 1. Snapshot tests for HA configuration. Integration test on multi-node Minikube: deploy with 3 replicas, verify pods spread across nodes, drain one node, verify PDB prevents complete disruption, verify new pod scheduled. HPA integration test: deploy with HPA, generate load, verify scale-up occurs, remove load, verify scale-down. Zero downtime test: rolling update from v1.22.0 to v1.23.0, monitor service availability, verify no connection failures.",
        "priority": "medium",
        "dependencies": [
          "5"
        ],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Create PodDisruptionBudget template with conditional enablement",
            "description": "Create templates/poddisruptionbudget.yaml with maxUnavailable: 1, selector matching deployment labels, enabled via podDisruptionBudget.enabled (default true when replicas > 1)",
            "dependencies": [],
            "details": "Create templates/poddisruptionbudget.yaml using policy/v1 apiVersion. Set maxUnavailable: {{ .Values.podDisruptionBudget.maxUnavailable | default 1 }}. Configure matchLabels selector to match deployment labels (app.kubernetes.io/name, app.kubernetes.io/instance). Add conditional rendering with {{- if .Values.podDisruptionBudget.enabled }} or automatic enablement when .Values.replicas > 1. Update values.yaml with podDisruptionBudget.enabled and podDisruptionBudget.maxUnavailable fields. Create unit test in tests/unit/poddisruptionbudget_test.yaml to verify correct maxUnavailable value, selector matching, and conditional enablement logic.",
            "status": "done",
            "testStrategy": "Unit tests verify PDB created only when enabled or replicas > 1, maxUnavailable matches configured value, selector matches deployment labels. Test with replicas=1 (PDB not created), replicas=3 (PDB created), explicit podDisruptionBudget.enabled=false (not created)."
          },
          {
            "id": 2,
            "title": "Create HorizontalPodAutoscaler template with CPU and memory metrics",
            "description": "Create templates/hpa.yaml with minReplicas, maxReplicas, targetCPUUtilizationPercentage (80), targetMemoryUtilizationPercentage (80), enabled via autoscaling.enabled",
            "dependencies": [],
            "details": "Create templates/hpa.yaml using autoscaling/v2 apiVersion. Configure spec.minReplicas: {{ .Values.autoscaling.minReplicas | default 2 }}, spec.maxReplicas: {{ .Values.autoscaling.maxReplicas | default 10 }}. Add metrics array with type: Resource, resource.name: cpu, resource.target.type: Utilization, resource.target.averageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage | default 80 }}. Add similar metric for memory utilization. Configure scaleTargetRef to reference the deployment. Add conditional rendering with {{- if .Values.autoscaling.enabled }}. Update values.yaml with autoscaling.enabled (default false), autoscaling.minReplicas, autoscaling.maxReplicas, autoscaling.targetCPUUtilizationPercentage, autoscaling.targetMemoryUtilizationPercentage. Create unit test in tests/unit/hpa_test.yaml.",
            "status": "done",
            "testStrategy": "Unit tests verify HPA created only when autoscaling.enabled=true, minReplicas and maxReplicas match configured values, CPU and memory target percentages are correct, scaleTargetRef points to correct deployment. Test with autoscaling.enabled=false (not created), custom metric values."
          },
          {
            "id": 3,
            "title": "Implement pod anti-affinity rules in deployment for zone distribution",
            "description": "Update templates/deployment.yaml with pod anti-affinity: preferredDuringSchedulingIgnoredDuringExecution, weight 100, topologyKey: topology.kubernetes.io/zone, labelSelector matching app labels",
            "dependencies": [],
            "details": "Update templates/deployment.yaml spec.template.spec.affinity.podAntiAffinity section. Add preferredDuringSchedulingIgnoredDuringExecution with weight: 100, podAffinityTerm.topologyKey: topology.kubernetes.io/zone, podAffinityTerm.labelSelector.matchLabels matching {{ include \"spicedb.selectorLabels\" . }}. Make anti-affinity configurable via values.yaml with affinity.podAntiAffinity.enabled (default true when replicas > 1), affinity.podAntiAffinity.topologyKey (default topology.kubernetes.io/zone), affinity.podAntiAffinity.weight (default 100). Support custom affinity overrides via values.affinity for advanced users. Create unit test in tests/unit/deployment_test.yaml to verify anti-affinity rules.",
            "status": "done",
            "testStrategy": "Unit tests verify podAntiAffinity section present when enabled, topologyKey is topology.kubernetes.io/zone, weight is 100, labelSelector matches deployment labels. Test with replicas=1 (anti-affinity disabled), replicas=3 (anti-affinity enabled), custom topologyKey and weight values, full affinity override."
          },
          {
            "id": 4,
            "title": "Add topology spread constraints for even pod distribution",
            "description": "Add topologySpreadConstraints to deployment with maxSkew 1, topologyKey: topology.kubernetes.io/zone, whenUnsatisfiable: ScheduleAnyway for balanced zone distribution",
            "dependencies": [],
            "details": "Update templates/deployment.yaml spec.template.spec.topologySpreadConstraints array. Add constraint with maxSkew: {{ .Values.topologySpreadConstraints.maxSkew | default 1 }}, topologyKey: {{ .Values.topologySpreadConstraints.topologyKey | default \"topology.kubernetes.io/zone\" }}, whenUnsatisfiable: {{ .Values.topologySpreadConstraints.whenUnsatisfiable | default \"ScheduleAnyway\" }}, labelSelector.matchLabels matching {{ include \"spicedb.selectorLabels\" . }}. Make configurable via values.yaml with topologySpreadConstraints.enabled (default true when replicas > 1), topologySpreadConstraints.maxSkew, topologySpreadConstraints.topologyKey, topologySpreadConstraints.whenUnsatisfiable. Support multiple topology spread constraints via array. Create unit test in tests/unit/deployment_test.yaml.",
            "status": "done",
            "testStrategy": "Unit tests verify topologySpreadConstraints present when enabled, maxSkew is 1, topologyKey is topology.kubernetes.io/zone, whenUnsatisfiable is ScheduleAnyway, labelSelector matches deployment labels. Test with replicas=1 (disabled), replicas=3 (enabled), custom maxSkew and topologyKey, multiple constraints."
          },
          {
            "id": 5,
            "title": "Configure rolling update strategy for zero-downtime deployments",
            "description": "Update deployment strategy to RollingUpdate with maxUnavailable: 0, maxSurge: 1 for zero downtime during updates",
            "dependencies": [
              1,
              2
            ],
            "details": "Update templates/deployment.yaml spec.strategy.type to RollingUpdate. Configure spec.strategy.rollingUpdate.maxUnavailable: {{ .Values.updateStrategy.rollingUpdate.maxUnavailable | default 0 }} and spec.strategy.rollingUpdate.maxSurge: {{ .Values.updateStrategy.rollingUpdate.maxSurge | default 1 }}. Ensure maxUnavailable: 0 to prevent any pods from being unavailable during updates. Add values.yaml fields: updateStrategy.type (default RollingUpdate), updateStrategy.rollingUpdate.maxUnavailable (default 0), updateStrategy.rollingUpdate.maxSurge (default 1). Create unit test in tests/unit/deployment_test.yaml to verify strategy configuration. Ensure PDB maxUnavailable is compatible with rolling update strategy (PDB maxUnavailable should be >= 1 when update maxUnavailable is 0).",
            "status": "done",
            "testStrategy": "Unit tests verify strategy.type is RollingUpdate, maxUnavailable is 0, maxSurge is 1. Test with custom updateStrategy values. Integration test: deploy with 3 replicas, trigger rolling update, verify at least 3 pods remain available throughout update, verify update completes successfully."
          },
          {
            "id": 6,
            "title": "Add resource requests and limits with configurable values",
            "description": "Add resources.requests (cpu: 500m, memory: 1Gi) and resources.limits (cpu: 2000m, memory: 4Gi) with configurable values in deployment",
            "dependencies": [],
            "details": "Update templates/deployment.yaml spec.template.spec.containers[0].resources section. Add resources.requests.cpu: {{ .Values.resources.requests.cpu | default \"500m\" }}, resources.requests.memory: {{ .Values.resources.requests.memory | default \"1Gi\" }}, resources.limits.cpu: {{ .Values.resources.limits.cpu | default \"2000m\" }}, resources.limits.memory: {{ .Values.resources.limits.memory | default \"4Gi\" }}. Update values.yaml with resources.requests.cpu, resources.requests.memory, resources.limits.cpu, resources.limits.memory with documented defaults. Ensure resources are set for HPA to function correctly (HPA requires resource requests). Create unit test in tests/unit/deployment_test.yaml to verify resource configuration.",
            "status": "done",
            "testStrategy": "Unit tests verify resources.requests.cpu is 500m, resources.requests.memory is 1Gi, resources.limits.cpu is 2000m, resources.limits.memory is 4Gi by default. Test with custom resource values. Test that HPA can be enabled when resources are defined."
          },
          {
            "id": 7,
            "title": "Create production HA example and conduct integration tests",
            "description": "Create examples/production-ha.yaml with full HA configuration and conduct integration tests on multi-node cluster with load testing and node drain scenarios",
            "dependencies": [
              1,
              2,
              3,
              4,
              5,
              6
            ],
            "details": "Create charts/spicedb/examples/production-ha.yaml demonstrating complete HA configuration: replicas: 3, podDisruptionBudget.enabled: true, autoscaling.enabled: true with minReplicas: 2 and maxReplicas: 10, resource requests and limits, anti-affinity and topology spread constraints enabled. Document the example with comments explaining each HA component. Create integration test script that: 1) deploys chart with production-ha.yaml on multi-node cluster (kind or real cluster), 2) generates load using hey or similar tool against gRPC and HTTP endpoints, 3) drains one node (kubectl drain) and verifies pods reschedule without service interruption, 4) scales deployment and verifies HPA behavior, 5) verifies PDB prevents excessive disruption during voluntary evictions. Document test results and HA validation approach in test output.\n<info added on 2025-11-08T05:21:46.248Z>\nProduction HA example successfully implemented in charts/spicedb/examples/production-ha.yaml (621 lines, 23KB) with comprehensive documentation. Example demonstrates all HA features: 3 replicas baseline, PodDisruptionBudget (maxUnavailable: 1), HorizontalPodAutoscaler (2-10 replicas, CPU/Memory 80% targets), resource requests (1000m CPU, 1Gi memory) and limits (2000m CPU, 2Gi memory), PostgreSQL datastore with SSL, preferred pod anti-affinity for node spreading, topology spread constraints (maxSkew: 1, zone distribution), zero-downtime rolling updates (maxUnavailable: 0, maxSurge: 1), Prometheus monitoring integration, security contexts, optional TLS configuration, and database migrations with resource limits.\n\nDocumentation includes detailed explanations of each feature, trade-off analysis (HPA vs static replicas, hard vs soft anti-affinity), cluster size recommendations, 10-step validation checklist, troubleshooting guide for common issues, and best practices for production deployments.\n\nVerified with helm template - successfully renders Deployment, HPA, PDB, RBAC, Service, and Secret resources with all configuration values properly applied.\n\nRemaining work: Create integration test script for multi-node cluster validation including: deployment of production-ha.yaml on kind/real cluster, load generation against gRPC/HTTP endpoints using hey or similar tool, node drain testing (kubectl drain) to verify pod rescheduling without service interruption, HPA scaling verification, PDB validation during voluntary evictions. Document test results and HA validation approach in test output.\n</info added on 2025-11-08T05:21:46.248Z>",
            "status": "done",
            "testStrategy": "Integration tests on multi-node cluster: deploy with production-ha.yaml, verify 3 replicas running, verify PDB created with maxUnavailable: 1, verify HPA created with correct metrics, verify pods distributed across zones/nodes via anti-affinity and topology spread. Load test: send continuous requests, drain node, verify <1s disruption or zero disruption, verify pods reschedule to other nodes. Scale test: increase load, verify HPA scales up to maxReplicas, decrease load, verify HPA scales down to minReplicas."
          }
        ],
        "complexity": 7,
        "recommendedSubtasks": 7,
        "expansionPrompt": "Divide into: 1) Create PodDisruptionBudget template with conditional enablement based on replica count, 2) Create HorizontalPodAutoscaler template with CPU and memory metrics, 3) Implement pod anti-affinity rules in deployment.yaml for zone distribution, 4) Add topology spread constraints for even distribution, 5) Configure rolling update strategy for zero-downtime deployments, 6) Add resource requests and limits with configurable values, 7) Create examples/production-ha.yaml and conduct integration tests on multi-node cluster with load testing and node drain scenarios"
      },
      {
        "id": 7,
        "title": "Implement Observability and Monitoring Integration",
        "description": "Add Prometheus ServiceMonitor, metrics endpoint configuration, structured logging, and pod label/annotation projection",
        "details": "Create templates/servicemonitor.yaml for Prometheus Operator with endpoint targeting port 9090 (metrics), path /metrics, interval 30s, enabled via monitoring.serviceMonitor.enabled (requires prometheus-operator CRDs). Update values.yaml with monitoring.enabled (default true), monitoring.serviceMonitor.enabled (default false), monitoring.serviceMonitor.interval, monitoring.serviceMonitor.scrapeTimeout, monitoring.serviceMonitor.labels (for Prometheus selection). Add SPICEDB_LOG_LEVEL environment variable (default: info, options: debug, info, warn, error). Add SPICEDB_LOG_FORMAT environment variable (default: json for structured logging). Add pod annotations for metrics scraping (prometheus.io/scrape, prometheus.io/port, prometheus.io/path) when monitoring.enabled. Add configurable pod labels via podLabels in values.yaml. Add configurable pod annotations via podAnnotations in values.yaml. Update service to expose metrics port 9090. Create observability documentation covering: Prometheus integration, ServiceMonitor setup, Grafana dashboard examples, log aggregation (stdout/stderr), common metrics to monitor (spicedb_check_duration, spicedb_datastore_queries).",
        "testStrategy": "ServiceMonitor tests verify endpoint configuration, port, path, interval. ServiceMonitor conditional tests verify created only when enabled and CRDs present. Metrics endpoint tests verify service exposes port 9090. Pod annotation tests verify prometheus.io/* annotations present when monitoring enabled. Log level tests verify SPICEDB_LOG_LEVEL environment variable set correctly. Log format tests verify structured logging enabled. Pod labels/annotations tests verify custom labels and annotations applied. Snapshot tests for monitoring configuration. Integration test: deploy with Prometheus Operator, verify ServiceMonitor created, verify metrics scraped, query sample metrics. Integration test without Prometheus Operator: verify chart installs successfully (ServiceMonitor optional).",
        "priority": "medium",
        "dependencies": [
          "6"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 5,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break into: 1) Create ServiceMonitor template with conditional CRD detection and Prometheus Operator integration, 2) Update values.yaml with monitoring configuration fields (intervals, labels, scrape timeout), 3) Add logging configuration (log level, log format) via environment variables, 4) Add pod annotations for metrics scraping and support custom pod labels/annotations, 5) Update service to ensure metrics port 9090 is exposed, 6) Create observability documentation covering Prometheus integration, Grafana dashboards, log aggregation, and key metrics to monitor"
      },
      {
        "id": 8,
        "title": "Implement Dispatch Cluster Mode Configuration",
        "description": "Enable distributed permission checking with Kubernetes service discovery and mTLS for inter-pod communication",
        "details": "Extend values.yaml with dispatch.enabled (default false), dispatch.upstreamCASecretName for custom CA certificates, dispatch.clusterName. Update templates/deployment.yaml to add dispatch environment variables when enabled: SPICEDB_DISPATCH_CLUSTER_ENABLED=true, SPICEDB_DISPATCH_UPSTREAM_ADDR using Kubernetes DNS (format: spicedb.namespace.svc.cluster.local:50053), SPICEDB_DISPATCH_UPSTREAM_CA_CERT_PATH when upstreamCASecretName provided. Mount upstream CA certificate secret as volume when dispatch.upstreamCASecretName set. Ensure dispatch port 50053 exposed in service (already in Phase 2). Update RBAC to include endpoints discovery permissions (already in Phase 2). When dispatch.enabled and tls.dispatch configured, verify mTLS environment variables set (from Phase 5). Add headless service option (service.headless: true, clusterIP: None) for StatefulSet future support. Document dispatch cluster configuration: when to enable, scaling considerations, network performance impact, mTLS requirements.",
        "testStrategy": "Dispatch environment variable tests verify SPICEDB_DISPATCH_CLUSTER_ENABLED set when enabled. Dispatch upstream address tests verify correct Kubernetes DNS format. Dispatch CA mounting tests verify volume and volumeMount created when upstreamCASecretName provided. Dispatch with mTLS tests: enable both dispatch and tls.dispatch, verify all TLS environment variables present. Service port tests verify dispatch port 50053 exposed. RBAC tests verify endpoints discovery permissions present. Snapshot tests for dispatch configuration. Integration test: deploy 3 replicas with dispatch enabled, verify inter-pod communication works, send check request, verify distributed query execution (check logs for dispatch events).",
        "priority": "medium",
        "dependencies": [
          "7"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Divide into: 1) Extend values.yaml with dispatch configuration fields (enabled, upstreamCASecretName, clusterName), 2) Add dispatch cluster environment variables to deployment.yaml with Kubernetes DNS-based service discovery, 3) Implement upstream CA certificate mounting when custom CA is provided, 4) Verify integration with existing TLS mTLS configuration from Task 5, 5) Add headless service option for future StatefulSet support, 6) Create integration tests with multi-replica deployment to verify inter-pod communication and create dispatch cluster documentation"
      },
      {
        "id": 9,
        "title": "Implement Ingress and NetworkPolicy Support",
        "description": "Add Ingress configuration for multiple controllers and NetworkPolicy templates for network segmentation",
        "details": "Create templates/ingress.yaml supporting multiple ingress controllers: nginx (kubernetes.io/ingress.class), Contour (projectcontour.io/ingress.class), Traefik (traefik.ingress.kubernetes.io/router.entrypoints). Support Ingress API versions: networking.k8s.io/v1 (default), with backward compatibility checks. Configuration via ingress.enabled, ingress.className, ingress.annotations, ingress.hosts (array with host, paths), ingress.tls (array with secretName, hosts). Support path types: Prefix, Exact, ImplementationSpecific. Support multiple hosts and paths. Create templates/networkpolicy.yaml with: ingress rules allowing traffic from ingress controller to SpiceDB ports (50051, 8443), ingress rules allowing inter-pod communication on dispatch port (50053), egress rules allowing DNS (port 53), datastore connectivity, Kubernetes API. Make NetworkPolicy optional via networkPolicy.enabled (default false). Add networkPolicy.ingress and networkPolicy.egress for custom rules. Create examples/production-ingress-nginx.yaml, examples/production-ingress-contour.yaml. Document Ingress setup for each controller type, TLS termination options (at ingress vs passthrough), NetworkPolicy security benefits and limitations.",
        "testStrategy": "Ingress tests verify correct apiVersion (networking.k8s.io/v1). Ingress className tests verify ingressClassName field set correctly. Ingress annotation tests for nginx, Contour, Traefik specific annotations. Ingress host/path tests verify multiple hosts and paths supported. Ingress TLS tests verify tls section with secretName and hosts. NetworkPolicy ingress tests verify rules allow traffic on SpiceDB ports. NetworkPolicy egress tests verify rules allow DNS and datastore connectivity. NetworkPolicy inter-pod tests verify dispatch port allowed. Snapshot tests for Ingress with each controller. Snapshot tests for NetworkPolicy. Integration test: deploy with nginx ingress, create Ingress resource, verify external access via hostname. Integration test: deploy with NetworkPolicy, verify policy applied, verify allowed traffic works, verify blocked traffic denied.",
        "priority": "low",
        "dependencies": [
          "8"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 6,
        "expansionPrompt": "Break into: 1) Create Ingress template supporting multiple controllers (nginx, Contour, Traefik) with networking.k8s.io/v1 API, 2) Implement multi-host and multi-path support with configurable path types, 3) Add TLS configuration for Ingress with certificate references, 4) Create NetworkPolicy template with ingress rules for controller and inter-pod traffic, 5) Add NetworkPolicy egress rules for DNS, datastore, and Kubernetes API access, 6) Create controller-specific examples (nginx, Contour) and integration tests with actual Ingress controllers and NetworkPolicy validation"
      },
      {
        "id": 10,
        "title": "Finalize Documentation, Examples, and Release Preparation",
        "description": "Complete comprehensive documentation, validate all examples, prepare Chart.yaml metadata, and ensure release readiness",
        "details": "Create comprehensive README.md with: overview, features, quick start (memory mode), installation instructions, configuration reference (all values.yaml fields), upgrade guide, troubleshooting section, contributing guidelines. Create QUICKSTART.md with 5-minute memory mode deployment. Create PRODUCTION_GUIDE.md with step-by-step PostgreSQL and CockroachDB production deployment, including prerequisites, infrastructure setup, TLS certificate generation, deployment steps, verification, integration. Create TROUBLESHOOTING.md with common issues: migration failures, TLS errors, connection issues, Pod Identity problems, OOM errors. Create UPGRADE_GUIDE.md with version upgrade paths, breaking changes, migration considerations, rollback procedures. Create SECURITY.md documenting security features, best practices, compliance considerations. Validate all example files: dev-memory.yaml, production-postgres.yaml, production-cockroachdb.yaml, production-cockroachdb-tls.yaml, production-ha.yaml, postgres-external-secrets.yaml, cert-manager-integration.yaml, production-ingress-nginx.yaml. Update Chart.yaml with: version (1.0.0), appVersion (latest SpiceDB version), description, home, sources, maintainers, keywords, annotations (artifacthub.io/*). Create CHANGELOG.md with release notes. Create LICENSE file (Apache 2.0). Run final validation: helm lint, helm unittest (all tests), conftest verify, helm template with all examples, helm package. Create .helmignore to exclude tests/, policies/, .github/, etc from package.",
        "testStrategy": "Documentation completeness review: verify all sections present, no broken links, code examples valid. Example validation: helm template each example file, verify no errors, verify rendered manifests valid. Chart.yaml validation: verify all required fields present, version follows semver. helm lint test: verify zero errors and warnings. helm unittest test: verify 90%+ coverage, all tests passing. Conftest test: verify all policies passing. helm package test: verify package created, correct size, no excluded files included. Integration test matrix: install chart with each example file on Minikube, verify successful deployment. Release checklist: all documentation complete, all tests passing, all examples validated, Chart.yaml complete, CHANGELOG.md updated, LICENSE present.",
        "priority": "medium",
        "dependencies": [
          "9"
        ],
        "status": "pending",
        "subtasks": [],
        "complexity": 6,
        "recommendedSubtasks": 8,
        "expansionPrompt": "Divide into: 1) Create comprehensive README.md with overview, features, quick start, installation, and configuration reference, 2) Create QUICKSTART.md for 5-minute memory mode deployment, 3) Create PRODUCTION_GUIDE.md with step-by-step PostgreSQL and CockroachDB deployment guides, 4) Create TROUBLESHOOTING.md with common issues and solutions, 5) Create UPGRADE_GUIDE.md and SECURITY.md documentation, 6) Validate all example files with helm template and verify rendered manifests, 7) Update Chart.yaml metadata, create CHANGELOG.md and LICENSE, create .helmignore, 8) Run final validation suite (helm lint, unittest, conftest, package) and complete release checklist"
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2025-11-08T04:56:35.882Z",
      "taskCount": 10,
      "completedCount": 3,
      "tags": [
        "master"
      ],
      "created": "2025-11-08T05:00:16.987Z",
      "description": "Tasks for master context",
      "updated": "2025-11-08T05:22:44.200Z"
    }
  }
}